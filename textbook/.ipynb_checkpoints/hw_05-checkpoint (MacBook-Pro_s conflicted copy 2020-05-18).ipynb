{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training models\n",
    "\n",
    "In this notebook you will learn about the following topics:\n",
    "\n",
    "- [Training models](#training)\n",
    "- [Gradient descent](#gradient_descent)\n",
    "- [Under and overfitting](#under_overfitting)\n",
    "- [Training, testing and validation sets](#train_test_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-0d5ab28632df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "\n",
    "We'll start by using the Boston housing dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/boston.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models <a id=\"training\"></a>\n",
    "\n",
    "As we learned last week, linear regression is the process of fitting a line to data in the \"best\" way possible. We measured \"best\" in terms of mean squared error (MSE), given specifically by\n",
    "$$\n",
    "\\displaystyle\\sum_i (y_i - (mx_i + b))^2\n",
    "$$\n",
    "where $y_i$ are the values to be predicted (median home value in our example last week) and $x_i$ are the data being used to make predictions (things like poverty rate, distance to downtown, etc). We then showed that you could simply take derivatives and find critical points to solve for what values of $m$ and $b$ will make this \"error\" as small as possible, i.e. minimize it. \n",
    "\n",
    "None of this is common in machine learning. In fact, linear regression is largely the only case of machine learning where we can actually *solve* for what value of the **model parameters**, the variables used in the model, will give the smallest error. Instead, we do what is called \"model training\".\n",
    "\n",
    "Model training works like this:\n",
    "1. Find data which you want your model to predict. \n",
    "2. Pick a model. Last week this was linear regression. As the semester goes on you'll learn about several other models.\n",
    "3. Start with random guesses for the model parameters.\n",
    "4. Have your model make predictions from your data, and compare the predictions to the correct values using a loss function like mean squared error.\n",
    "5. Take the gradient of the loss function and adjust the model parameters in the direction of negative of the gradient.\n",
    "6. Repeat steps 4 through 6 over and over.\n",
    "\n",
    "Let's go through these steps one-by-one, as each requires significant explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding data to train your model on\n",
    "The first step is to find data. Normally you've got a general problem in mind that you want to answer. Your first step should be looking for data related to that problem. If you don't have data then you can't do anything else either.\n",
    "\n",
    "Let's define a few terms that we will be using throughout the rest of this semester:\n",
    "- **features:** Features are simply the \"inputs\" in your data. So in the Titanic example, this would be things like age, fare, sex, etc.\n",
    "- **labels:** Labels are the values you want to predict. The term \"label\" comes from when you are trying to predict a categorical variable, such as the breed of a dog, or the survival or death of a passenger. However we also use it for numerical variables, such as home value.\n",
    "- **ground truth:** This refers to the \"correct\" values of the labels. For instance, suppose we collected data on passengers on the Titanic. We could build a machine learning model to predict whether or not each person survived, and the model would predict a label (\"survived\" or \"died\"). However, these are just *predictions*. By \"ground truth\" we mean the actual correct labels. That is, for each person described in the data, did they *actually* survive or die? Whatever the answer to this is is called the ground truth.\n",
    "\n",
    "So we want data with features and ground truth. Once we have that, we can move on to step 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model types\n",
    "\n",
    "Machine learning \"models\" are simply functions. Linear regression is an especially simple function represented by a simple equation ($y=mx+b$). When dealing with inputs with many features then $m$, $x$ and $b$ are all vectors, which makes things seem complicated. But in reality it's just a line. Another model we will deal with extensively this semester is called a \"decision tree\". We will hold off on the details for now, but a decision tree is simply a function that repeated asks \"yes/no\" questions of the data. For instance, suppose we want to use a decision tree to determine whether or not a passenger on the titanic survives. Below is a possible decision tree:\n",
    "\n",
    "![Titanic decision tree](images/titanic_decision_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the first question is about the person's gender, then if they are a female the model predicts they will survive. If they are a male, the model then asks about their age, and so forth. This may not *look* like a function, but it is. Recall that a function is simply something that takes in input and returns a single output (think about the \"vertical line test\"). We could write this as an *equation* (which is probably how you typically think about functions) as follows:\n",
    "$$\n",
    "f(\\text{sex}, \\text{age}, \\text{sibsp}) = \\text{piecewise function}\n",
    "$$\n",
    "In this decision tree we have the following model parameters:\n",
    "- Which columns should we ask questions about? \n",
    "- What order should we ask these questions? Do we start with sex, age, or sibsp?\n",
    "- When we ask about a numerical column (such as \"is age > 9.5\" or \"is sibsp > 2.5\"), what value should be our cutoff? That is why aren't we asking \"is age > 14\", or \"is sibsp < 5\"?\n",
    "- After each question, what should we do next? Should we predict a value or go to another question?\n",
    "- Whenever we decide to predict a value, what value should we predict?\n",
    "\n",
    "As you can see, model parameters can be quite complicated. It is impossible to setup an equation and \"solve\" for each of these like we did for linear regression. So instead, we train the model. That leads us to step 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with random guesses for the model parameters\n",
    "\n",
    "Suppose you decide to predict home value using the data in the Boston dataset. You decide to use linear regression, and so know your model will look something like\n",
    "$$\n",
    "y = m_1\\cdot \\text{crime_rate} + m_2\\cdot \\text{pct_industrial} + \\cdots + m_8\\cdot \\text{poverty} + b\n",
    "$$\n",
    "What values of $m_1, m_2, \\ldots, m_8, b$ will make the predicted value be as close as possible to the ground truth? You have no idea? So all you can do is start with a random guess. Let's have numpy do this for us using `np.random`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 8 random \"slopes\"\n",
    "m = np.random.rand(8)\n",
    "\n",
    "# Pick one random \"y-intercept\"\n",
    "b = np.random.rand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57864073, 0.2565169 , 0.65679831, 0.06675287, 0.41239833,\n",
       "       0.13474376, 0.53575606, 0.97992396])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.76140737])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we've got our initial guess at a function, given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y = 0.579(Median home value) + 0.257(Crime rate) + 0.657(% industrial) + 0.067(Nitrous Oxide concentration) + 0.412(Avg num rooms) + 0.135(% built before 1940) + 0.536(Distance to downtown) + 0.980(Pupil-teacher ratio) + 0.761'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fancy code to dynamically generate the equation\n",
    "'y = ' + ' + '.join(f'{m:.3f}({x})' for m, x in list(zip(m, df.columns))) + f' + {b[0]:.3f}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this give a good prediction? Probably not! Let's try a quick example. We'll first write a function which makes a line out of this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_lr(x, m, b):\n",
    "    # np.dot is the dot product, so multiply and add\n",
    "    return np.dot(m, x) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 42.67, Actual: 24000.00\n"
     ]
    }
   ],
   "source": [
    "features_df = df[['Crime rate', '% industrial', 'Nitrous Oxide concentration', 'Avg num rooms', '% built before 1940', \n",
    "                 'Distance to downtown', 'Pupil-teacher ratio', '% below poverty line']]\n",
    "\n",
    "predicted_home_value = my_lr(x=features_df.iloc[0], m=m, b=b)[0] # This returns a list with the home price, we \"pull it out\" using [0]\n",
    "actual_home_value = df['Median home value'].iloc[0]\n",
    "\n",
    "print(f'Predicted: {predicted_home_value:.2f}, Actual: {actual_home_value:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowhere close! But that's not surprising, we just started with random guesses for the slopes and y-intercept. However, by comparing what we predicted to the ground truth we can improve on our guesses using something called \"gradient descent\". We will go into gradient descent later in this notebook, but for now let's just summarize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "Recall from Calculs 3 that the \"gradient\" is a function which takes the partial derivative with respect to each variable, and evaluates it at the current point. Since the gradient is a real-valued vector, so we can visualize it. In particular, the vector *points in the direction of greatest increase of the function*. So for example, if we take a very simple example like $f(x, y) = x^2 + y^2$, this defines a 3d surface. The gradient is given by $(2x, 2y)$, and evaluating it at (say), $(x, y) = (1, 2)$ we get the vector $(2, 4)$. How should we think about this vector? **MORE HERE**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update your parameters\n",
    "\n",
    "Now that we know which \"direction\" to move the values of our parameters to make the loss smaller, we'll do that. This means that our model with these new parameters now makes a better prediction. **SHOW EXAMPLE CALCULATIONS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under- and over-fitting\n",
    "\n",
    "Now that you know how model training works, let's do an example in code. We'll be using a type of model that you haven't worked with yet, but will be more and more important as the semester goes along. This model is called a \"Decision Tree\", and the tree diagram at the top of this notebook is one such example of one. Sklearn has a decision tree model which we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
