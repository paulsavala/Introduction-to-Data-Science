
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Linear regression &#8212; Introduction to Data Science</title>
    
  <link rel="stylesheet" href="_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Training models" href="ch_05.html" />
    <link rel="prev" title="Correlation" href="ch_03.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  
  <h1 class="site-logo" id="site-title">Introduction to Data Science</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="ch_01.html">
   First steps with Pandas and Jupyter notebooks
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="ch_02.html">
   Plotting and grouping data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ch_03.html">
   Correlation
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Linear regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ch_05.html">
   Training models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ch_06.html">
   Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ch_07.html">
   Nearest neighbors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ch_08.html">
   Decision Trees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ch_09.html">
   Improving your model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ch_10.html">
   Ensemble tree models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ch_11.html">
   Working with large datasets
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/ch_04.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/ch_04.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#imports">
   Imports
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-linear-regression-a-id-what-is-lr-a">
   What is linear regression?
   <a id="what_is_lr">
   </a>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-loading">
     Data loading
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-cleaning">
     Data cleaning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#an-example">
     An example
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-mathematics-of-linear-regression-a-id-math-lr-a">
   The mathematics of linear regression
   <a id="math_lr">
   </a>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-regression-using-scikit-learn-a-id-lr-sklearn-a">
   Linear regression using scikit-learn
   <a id="lr_sklearn">
   </a>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-regression-with-just-one-variable">
     Linear regression with just one variable
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-descent-a-id-gradient-descent-a">
   Gradient descent
   <a id="gradient_descent">
   </a>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="linear-regression">
<h1>Linear regression<a class="headerlink" href="#linear-regression" title="Permalink to this headline">¶</a></h1>
<p>Linear regression is perhaps the most important idea in machine learning. Once you understand the ins-and-outs of linear regression you can understand most other machine learning models as well. This is because the ideas developed for machine learning were first perfected on linear regression and then applied to other models. Let’s jump in.</p>
<div class="section" id="imports">
<h2>Imports<a class="headerlink" href="#imports" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="what-is-linear-regression-a-id-what-is-lr-a">
<h2>What is linear regression? <a id="what_is_lr"></a><a class="headerlink" href="#what-is-linear-regression-a-id-what-is-lr-a" title="Permalink to this headline">¶</a></h2>
<p>Linear regression (LR) is what you have probably referred to as “line of best fit”. It is a line meant to fit the data “as well as possible”. I put that last phrase in quotes, because what exactly do we mean by a “best fit”? We will formulate this mathematically in the next section.</p>
<p>With that out of the way, we now turn to our next question: <em>why</em> do we care about linear regression? Linear regression is extremely important because it allows us to <em>make predictions</em>. Up until this point we have only explored and described <em>the past</em> by looking at datasets which (necessarily) had data about the past. However, <strong>the point of data science is largely to make predictions about the future</strong> using data from the past. This works because a line doesn’t care what data we plug in. We can plug in data from the past in order to verify and explain past performance. But we can also plug in <em>future numbers</em> (dates, pricing changes, expected changes to our products, etc.) and see what the model returns.</p>
<p>Let’s start with a simple example. Don’t worry about the code right now, just look at the graphs. We will work again with our <code class="docutils literal notranslate"><span class="pre">data/boston.csv</span></code> dataset, describing the median home value in various neighborhoods in Boston.</p>
<div class="section" id="data-loading">
<h3>Data loading<a class="headerlink" href="#data-loading" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/boston.csv&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Median home value</th>
      <th>Crime rate</th>
      <th>% industrial</th>
      <th>Nitrous Oxide concentration</th>
      <th>Avg num rooms</th>
      <th>% built before 1940</th>
      <th>Distance to downtown</th>
      <th>Pupil-teacher ratio</th>
      <th>% below poverty line</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>24000.0</td>
      <td>0.00632</td>
      <td>2.31</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>15.3</td>
      <td>4.98</td>
    </tr>
    <tr>
      <th>1</th>
      <td>21600.0</td>
      <td>0.02731</td>
      <td>7.07</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>17.8</td>
      <td>9.14</td>
    </tr>
    <tr>
      <th>2</th>
      <td>34700.0</td>
      <td>0.02729</td>
      <td>7.07</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>17.8</td>
      <td>4.03</td>
    </tr>
    <tr>
      <th>3</th>
      <td>33400.0</td>
      <td>0.03237</td>
      <td>2.18</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.8</td>
      <td>6.0622</td>
      <td>18.7</td>
      <td>2.94</td>
    </tr>
    <tr>
      <th>4</th>
      <td>36200.0</td>
      <td>0.06905</td>
      <td>2.18</td>
      <td>0.458</td>
      <td>7.147</td>
      <td>54.2</td>
      <td>6.0622</td>
      <td>18.7</td>
      <td>5.33</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</div>
<div class="section" id="data-cleaning">
<h3>Data cleaning<a class="headerlink" href="#data-cleaning" title="Permalink to this headline">¶</a></h3>
<p>Change all column names to lowercase, convert spaces to underscores, replace “%” with “pct”, replace “-” with an underscore, check for missing values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="s1">&#39;_&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;%&#39;</span><span class="p">,</span> <span class="s1">&#39;pct&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="s1">&#39;_&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>median_home_value</th>
      <th>crime_rate</th>
      <th>pct_industrial</th>
      <th>nitrous_oxide_concentration</th>
      <th>avg_num_rooms</th>
      <th>pct_built_before_1940</th>
      <th>distance_to_downtown</th>
      <th>pupil_teacher_ratio</th>
      <th>pct_below_poverty_line</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>24000.0</td>
      <td>0.00632</td>
      <td>2.31</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>15.3</td>
      <td>4.98</td>
    </tr>
    <tr>
      <th>1</th>
      <td>21600.0</td>
      <td>0.02731</td>
      <td>7.07</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>17.8</td>
      <td>9.14</td>
    </tr>
    <tr>
      <th>2</th>
      <td>34700.0</td>
      <td>0.02729</td>
      <td>7.07</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>17.8</td>
      <td>4.03</td>
    </tr>
    <tr>
      <th>3</th>
      <td>33400.0</td>
      <td>0.03237</td>
      <td>2.18</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.8</td>
      <td>6.0622</td>
      <td>18.7</td>
      <td>2.94</td>
    </tr>
    <tr>
      <th>4</th>
      <td>36200.0</td>
      <td>0.06905</td>
      <td>2.18</td>
      <td>0.458</td>
      <td>7.147</td>
      <td>54.2</td>
      <td>6.0622</td>
      <td>18.7</td>
      <td>5.33</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">isna</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>median_home_value              0
crime_rate                     0
pct_industrial                 0
nitrous_oxide_concentration    0
avg_num_rooms                  0
pct_built_before_1940          0
distance_to_downtown           0
pupil_teacher_ratio            0
pct_below_poverty_line         0
dtype: int64
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="an-example">
<h3>An example<a class="headerlink" href="#an-example" title="Permalink to this headline">¶</a></h3>
<p>Don’t worry about the code/details right now. Instead, let’s just jump into an example showing what linear regression looks like, in case you’re not familiar with it. While you shouldn’t stress about the code, I encourage you to look through it and try and guess what each line is doing.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;pct_below_poverty_line&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;median_home_value&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;pct_below_poverty_line&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;median_home_value&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Actual&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;scatter&#39;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predicted&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;% below poverty line&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Median home value&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Poverty vs median home value&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ch_04_14_0.png" src="_images/ch_04_14_0.png" />
</div>
</div>
<p>In blue is the actual measurements of poverty and home value for all neighborhoods. In red is the predicted value using the line of best fit. We can see that the regression line seems to fit the data fairly well, at least in all except the far left and right ends.</p>
<p>One interesting thing to note is that the regression line generally seems too high. For example, if we draw the same graph, but only keep poverty values between 5% and 20% we get the following:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;pct_below_poverty_line&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;median_home_value&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Actual&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;scatter&#39;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predicted&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;% below poverty line&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Median home value&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Poverty vs median home value&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ch_04_16_0.png" src="_images/ch_04_16_0.png" />
</div>
</div>
<p>Why is that? The reason is that the regression line is heavily affected by outliers. So the neighborhoods with low crime and high home value are throwing off the line and “dragging it up.” In general, you want the following four things to be true before using LR for making predictions:</p>
<ol class="simple">
<li><p>The data should be approximately linear</p></li>
<li><p>The observations should be independent (so the crime rate and median home value in one neighborhood should be independent of other neighborhoods)</p></li>
<li><p>the variance between the measurements should be approximately the same throughout the graph (the graph is more or less spread out the same amount in different areas)</p></li>
<li><p>The points should be approximately normally distributed around the regression line.</p></li>
</ol>
<p>None of these four are perfectly satisfied. However, that doesn’t mean you <em>can’t</em> use LR. It just means that you need to be careful when making predictions. Don’t just make a regression line and say “see, this predicts the future!” Use your brain, that’s what it’s there for.</p>
</div>
</div>
<div class="section" id="the-mathematics-of-linear-regression-a-id-math-lr-a">
<h2>The mathematics of linear regression <a id="math_lr"></a><a class="headerlink" href="#the-mathematics-of-linear-regression-a-id-math-lr-a" title="Permalink to this headline">¶</a></h2>
<p>In the example above, the equation of the regression line was given by <span class="math notranslate nohighlight">\(y=-950.05x + 34553.84\)</span> (we’ll explain how to see this later). How did those numbers come about? How did Python (or more specifically sklearn) decide that those parameters made the line “best fit” the data? What do we even mean by “best fit”?</p>
<p>To answer this, let’s start by coming up with a way to measure our model being <em>wrong</em>. One simple way might be to take each point, see what the model predicts for that point, then take the difference. For example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;median_home_value&#39;</span><span class="p">,</span> <span class="s1">&#39;pct_below_poverty_line&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>median_home_value         24000.00
pct_below_poverty_line        4.98
Name: 0, dtype: float64
</pre></div>
</div>
</div>
</div>
<p>So in this neighborhood, 4.98% of the residents are below the poverty line, and the median home value is $24,000 (this is from Boston in the 1980’s). What does our linear model predict the home value would be for this poverty rate? Again, don’t worry about the code right now, we’ll get to that.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="si">{</span><span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">4.98</span><span class="p">]])[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>$29822.60
</pre></div>
</div>
</div>
</div>
<p>That’s a difference of $29,822.60 - $24,000 = $5,822.60 (or negative, depending on which way you subtract). In other words, our model was wrong by a little under six thousand dollars. Let’s repeat that for the first ten measurements.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">actual_x</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;pct_below_poverty_line&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">actual_y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;median_home_value&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">predicted_y</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">actual_x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">actual_y</span> <span class="o">-</span> <span class="n">predicted_y</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Actual = </span><span class="si">{</span><span class="n">actual_y</span><span class="si">}</span><span class="s1">, Predicted = </span><span class="si">{</span><span class="n">predicted_y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">, Diff = </span><span class="si">{</span><span class="n">diff</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Actual = 24000.0, Predicted = 29822.60, Diff = -5822.60
Actual = 21600.0, Predicted = 25870.39, Diff = -4270.39
Actual = 34700.0, Predicted = 30725.14, Diff = 3974.86
Actual = 33400.0, Predicted = 31760.70, Diff = 1639.30
Actual = 36200.0, Predicted = 29490.08, Diff = 6709.92
Actual = 28700.0, Predicted = 29604.08, Diff = -904.08
Actual = 22900.0, Predicted = 22744.73, Diff = 155.27
Actual = 27100.0, Predicted = 16360.40, Diff = 10739.60
Actual = 16500.0, Predicted = 6118.86, Diff = 10381.14
Actual = 18900.0, Predicted = 18308.00, Diff = 592.00
</pre></div>
</div>
</div>
</div>
<p>Let’s now write this mathematically. Let <span class="math notranslate nohighlight">\(y\)</span> be the actual y-value (median home value, in this case) and <span class="math notranslate nohighlight">\(\hat{y}\)</span> (pronounced “y-hat”) be the predicted home value (using our linear regression model, in this case). Then we’re looking at <span class="math notranslate nohighlight">\(\displaystyle\sum_{i=0}^N y_i - \hat{y_i}\)</span>. Since our predicted y values are given by a line, the equation for <span class="math notranslate nohighlight">\(\hat{y}\)</span> is <span class="math notranslate nohighlight">\(\hat{y}=mx+b\)</span>, so let’s plug that in too.</p>
<div class="math notranslate nohighlight">
\[\displaystyle\sum_{i=0}^N y_i - (mx_i+b)\]</div>
<p>Remember, all this is doing is taking the actual home value and subtracting the predicted home value. This will be zero if the predictions are perfect (i.e. they always equal the actual home value). The problem with this is the following: Suppose one neighborhood has an actual home value of $10,000 and a predicted value of $15,000. Then the difference is <span class="math notranslate nohighlight">\(y - \hat{y} = 10000 - 15000 = -5000\)</span>. Now suppose another neighborhood has an actual home value of $20,000 and a predicted value of $15,000. Then the difference is <span class="math notranslate nohighlight">\(y - \hat{y} = 20000 - 15000 = 5000\)</span>. Adding these together gives us <span class="math notranslate nohighlight">\(-5000 + 5000 = 0\)</span>, a perfect pair of predictions!</p>
<p>But wait, this is “perfect”? One predicted $5,000 too high, and the other $5,000 too low. It just so happened that these two mistakes cancelled out, and it made our prediction look perfect. Obviously we don’t want that. One solution would be to take the absolute value of the difference and add them up.</p>
<div class="math notranslate nohighlight">
\[\displaystyle\sum_{i=0}^N \mid y_i - (mx_i+b) \mid\]</div>
<p>We will call this our <strong>loss function</strong>, since it measures how far off our model is from being perfect. A “perfect model” (i.e. one in which every prediction is exactly equal to the actual value) should have a loss function which returns zero. Since there is generally no “perfect model”, we’ll think of this more generally: <em>A model which makes predictions which are close to being correct (i.e. matching real data) will have a loss function returning small values. A model which makes predictions which are far from correct will have a loss function returning large values.</em> What exactly we mean by “small” and “large” is not clear. It can only be assessed in relationship to each other.</p>
<p>Let’s keep exploring what we have. Let’s write a function which computes the loss function from the first <span class="math notranslate nohighlight">\(N\)</span> data points in our dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">loss_value</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">x_col</span><span class="o">=</span><span class="s1">&#39;pct_below_poverty_line&#39;</span><span class="p">,</span> <span class="n">y_col</span><span class="o">=</span><span class="s1">&#39;median_home_value&#39;</span><span class="p">):</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="n">actual_x</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">x_col</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">actual_y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">y_col</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">predicted_y</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">actual_x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">diff</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">actual_y</span> <span class="o">-</span> <span class="n">predicted_y</span><span class="p">)</span> <span class="c1"># Absolute value</span>
    <span class="k">return</span> <span class="n">diff</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Now let’s test it out with a few data points.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss_value</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>22417.069296777354
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss_value</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>45189.1692244646
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss_value</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>383234.58817355137
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss_value</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>809654.8108495052
</pre></div>
</div>
</div>
</div>
<p>Let’s graph the loss values to see how they change as <span class="math notranslate nohighlight">\(N\)</span> gets bigger.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">loss_values</span> <span class="o">=</span> <span class="p">[</span><span class="n">loss_value</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">)]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">loss_values</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Loss values&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;N&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ch_04_32_0.png" src="_images/ch_04_32_0.png" />
</div>
</div>
<p>Notice our loss value keeps getting bigger and bigger. This is because we’re adding up more and more errors. That’s a problem, because it makes it look like the model is worse and worse, just because we tested it on more and more data. Any prediction which isn’t perfect will make our loss value higher, and thus make our model seem “worse”. Let’s fix that by taking an <em>average</em>. That means it will only see <em>on average</em> how far off our predictions are.</p>
<div class="math notranslate nohighlight">
\[\frac{1}{N} \displaystyle\sum_{i=0}^N \mid y_i - (mx_i+b) \mid\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">loss_value_avg</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">x_col</span><span class="o">=</span><span class="s1">&#39;pct_below_poverty_line&#39;</span><span class="p">,</span> <span class="n">y_col</span><span class="o">=</span><span class="s1">&#39;median_home_value&#39;</span><span class="p">):</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="n">actual_x</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">x_col</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">actual_y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">y_col</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">predicted_y</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">actual_x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">diff</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">actual_y</span> <span class="o">-</span> <span class="n">predicted_y</span><span class="p">)</span> <span class="c1"># Absolute value</span>
    <span class="k">return</span> <span class="n">diff</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">N</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss_value_avg</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4483.413859355471
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss_value_avg</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4518.9169224464595
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss_value_avg</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3832.3458817355136
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss_value_avg</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4048.2740542475262
</pre></div>
</div>
</div>
</div>
<p>And how about a graph to illustrate this.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span> <span class="p">[</span><span class="n">loss_value_avg</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">200</span><span class="p">)])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x7f2663c5d320&gt;]
</pre></div>
</div>
<img alt="_images/ch_04_40_1.png" src="_images/ch_04_40_1.png" />
</div>
</div>
<p>Much better! Now we can see that our model generally has a loss value around 3,400 to 4,000. Is this good? We can’t really say without comparing it to other models, but for now let’s now worry about it.</p>
<p>This seems to answer our initial question of “what do we mean by a good model?”. What we mean is a model which has a low loss value. We now turn to our second question: How do we find which parameters of the model (slope and y-intercept in the case of a linear model) give a good model? Should the slope be 50? 100? -15? How can we know? If our goal is to make the loss value be as small as possible, then in fact we do know! Calculus tells us that to make a function as small as possible (that is, to find its <em>minimum</em>) we should find its critical points. Let’s write the loss function as a <em>function</em> and later we’ll try differentiating it.</p>
<div class="math notranslate nohighlight">
\[L(m, b) = \frac{1}{N} \displaystyle\sum_{i=0}^N \Bigl\lvert y_i - (mx_i+b) \Bigr\rvert\]</div>
<p>Notice that we have two variables, <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(b\)</span>. These may not be the variables you would have guessed from looking at the question. You may have thought it was <span class="math notranslate nohighlight">\(y_i\)</span>, <span class="math notranslate nohighlight">\(x_i\)</span> or <span class="math notranslate nohighlight">\(N\)</span>. But remember that <span class="math notranslate nohighlight">\(y_i\)</span>, <span class="math notranslate nohighlight">\(x_i\)</span> and <span class="math notranslate nohighlight">\(N\)</span> are not <em>variables</em>, they are actual numbers from our data. Remember that <span class="math notranslate nohighlight">\(y_i\)</span> is simply the median home value of the <span class="math notranslate nohighlight">\(i\)</span>th row, <span class="math notranslate nohighlight">\(x_i\)</span> is the poverty rate of the <span class="math notranslate nohighlight">\(i\)</span>th row, and <span class="math notranslate nohighlight">\(N\)</span> is the number of data points in our dataset. There’s nothing “variable” about them, they are whatever they are. However, what we <em>don’t</em> know values for are the slope and y-intercept. That’s why <em>these</em> are the variables.</p>
<p>Let’s now take the (partial) derivatives with respect to these. If you have a hard time thinking about derivatives with so many letters involved, just try a simple little example. Suppose the y-values are <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">3]</span></code> and the x-values are <code class="docutils literal notranslate"><span class="pre">[4,</span> <span class="pre">5,</span> <span class="pre">6]</span></code>. Then this looks like</p>
<div class="math notranslate nohighlight">
\[L(m, b) = \frac{1}{N} \displaystyle\sum_{i=0}^N \Bigl\lvert y_i - (mx_i+b) \Bigr\rvert = \frac13 \Bigl\lvert (1 - (4x+b)) + (2 - (5x+b)) + (3 - (6x+b)) \Bigr\rvert  \]</div>
<p>How would you take the derivative of this? Absolute values are tricky, and require splitting up as a piecewise function. It sure would be nice if we didn’t have to deal with that. In addition, the derivative doesn’t exist at zero, so that could be an issue. Wouldn’t it be nice if there was a function which only returned positive values (like absolute value), but which was easy to take the derivative of (<em>unlike</em> absolute value)…</p>
<p>Squaring! That is exactly what squaring does. It is simple to take the derivative of, and only returns positive values. Let’s use that to redefine our loss function:</p>
<div class="math notranslate nohighlight">
\[L(m, b) = \frac{1}{N} \displaystyle\sum_{i=0}^N \Bigl( y_i - (mx_i+b) \Bigr)^2\]</div>
<p>We’ll refer to this equation as <strong>mean squared error</strong>, or <strong>MSE</strong> for short.</p>
<p>Next, let’s use what we know about linear algebra and matrices to rewrite this. Let <span class="math notranslate nohighlight">\(X\)</span> be the matrix of <span class="math notranslate nohighlight">\(x\)</span> values. Each row is one measurement (a neighborhood in our housing example). In general we may have many measurements for <span class="math notranslate nohighlight">\(X\)</span>, such as using <code class="docutils literal notranslate"><span class="pre">pct_below_poverty_line</span></code>, <code class="docutils literal notranslate"><span class="pre">pupil_teacher_ratio</span></code> and <code class="docutils literal notranslate"><span class="pre">distance_to_downtown</span></code> all to predict <code class="docutils literal notranslate"><span class="pre">median_home_value</span></code>. That’s why it’s a matrix. Let <span class="math notranslate nohighlight">\(y\)</span> be the column vector of <span class="math notranslate nohighlight">\(y\)</span> values. So in our example <span class="math notranslate nohighlight">\(y\)</span> would be the median home values. <span class="math notranslate nohighlight">\(y\)</span> is a <em>vector</em> and not a <em>matrix</em> (like <span class="math notranslate nohighlight">\(X\)</span>) because we’re always predicting a <em>number</em>, just a single value. Finally, <span class="math notranslate nohighlight">\(m\)</span> is a column vector of slopes, one for each column. Finally, <span class="math notranslate nohighlight">\(b\)</span> is a column vector (check the dimensions for yourself to make sure everything matches up). For a simple example, let’s take the first three rows of data below and write out exactly what we have:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;median_home_value&#39;</span><span class="p">,</span> <span class="s1">&#39;pct_below_poverty_line&#39;</span><span class="p">,</span> <span class="s1">&#39;pupil_teacher_ratio&#39;</span><span class="p">,</span> <span class="s1">&#39;distance_to_downtown&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>median_home_value</th>
      <th>pct_below_poverty_line</th>
      <th>pupil_teacher_ratio</th>
      <th>distance_to_downtown</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>24000.0</td>
      <td>4.98</td>
      <td>15.3</td>
      <td>4.0900</td>
    </tr>
    <tr>
      <th>1</th>
      <td>21600.0</td>
      <td>9.14</td>
      <td>17.8</td>
      <td>4.9671</td>
    </tr>
    <tr>
      <th>2</th>
      <td>34700.0</td>
      <td>4.03</td>
      <td>17.8</td>
      <td>4.9671</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="math notranslate nohighlight">
\[\begin{split}y = \begin{pmatrix}
24000 \\
21600 \\
34700
\end{pmatrix} \hspace{0.5cm}
X = \begin{pmatrix}
4.98 &amp; 15.3 &amp; 4.09 \\
9.14 &amp; 17.8 &amp; 4.9671 \\
4.03 &amp; 17.8 &amp; 4.9671
\end{pmatrix}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}m = \begin{pmatrix}
m_{\text{poverty}} \\
m_{\text{pupil}} \\
m_{\text{downtown}}
\end{pmatrix} \hspace{0.5cm}
b = \begin{pmatrix}
b_1 \\
b_2 \\
b_3
\end{pmatrix}\end{split}\]</div>
<p>This gives the equation</p>
<div class="math notranslate nohighlight">
\[L(m, b) = \frac{1}{N}(y - Xm - b)^2\]</div>
<p>So, we’re trying to pick values of <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(b\)</span> to make the two sides of the following equation as close as possible:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{pmatrix}
24000 \\
21600 \\
34700
\end{pmatrix} 
\approx
\begin{pmatrix}
4.98 &amp; 15.3 &amp; 4.09 \\
9.14 &amp; 17.8 &amp; 4.9671 \\
4.03 &amp; 17.8 &amp; 4.9671
\end{pmatrix}
\begin{pmatrix}
m_{\text{poverty}} \\
m_{\text{pupil}} \\
m_{\text{downtown}}
\end{pmatrix}
+
\begin{pmatrix}
b_1 \\
b_2 \\
b_3
\end{pmatrix}\end{split}\]</div>
<p>or equivalently</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
24000 &amp;\approx 4.98\cdot m_{poverty} + 15.3\cdot m_{pupil} + 4.09\cdot m_{downtown} + b_1 \\ 
21600 &amp;\approx 9.14\cdot m_{poverty} + 17.8\cdot m_{pupil} + 4.9671\cdot m_{downtown} + b_2 \\ 
34700 &amp;\approx 4.03\cdot m_{poverty} + 17.8\cdot m_{pupil} + 4.9671\cdot m_{downtown} + b_3
\end{align}\end{split}\]</div>
<p>This is a system of equation in three variables which can be solved (assuming the system isn’t singular). This simplest way to solve it is to include the <span class="math notranslate nohighlight">\(b_i\)</span> terms into the <span class="math notranslate nohighlight">\(X\)</span> matrix. We can do noting that</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{pmatrix}
4.98 &amp; 15.3 &amp; 4.09 \\
9.14 &amp; 17.8 &amp; 4.9671 \\
4.03 &amp; 17.8 &amp; 4.9671
\end{pmatrix}
\begin{pmatrix}
m_{\text{poverty}} \\
m_{\text{pupil}} \\
m_{\text{downtown}}
\end{pmatrix}
+
\begin{pmatrix}
b_1 \\
b_2 \\
b_3
\end{pmatrix}
=
\begin{pmatrix}
4.98 &amp; 15.3 &amp; 4.09 &amp; b_1\\
9.14 &amp; 17.8 &amp; 4.9671 &amp; b_2\\
4.03 &amp; 17.8 &amp; 4.9671 &amp; b_3
\end{pmatrix}
\begin{pmatrix}
m_{\text{poverty}} \\
m_{\text{pupil}} \\
m_{\text{downtown}} \\
1
\end{pmatrix}\end{split}\]</div>
<p>We’ll call this augmented <span class="math notranslate nohighlight">\(X\)</span> matrix <span class="math notranslate nohighlight">\(X_b\)</span>, and the augmented <span class="math notranslate nohighlight">\(m\)</span> vector <span class="math notranslate nohighlight">\(m_1\)</span>. For simplicity, we’ll just call these <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(m\)</span>. Then more generally we can write</p>
<div class="math notranslate nohighlight">
\[L(m, b) = \frac{1}{N}(y - X m)^2\]</div>
<p>Note that <span class="math notranslate nohighlight">\(y-X m\)</span> is a vector (a column vector with the same dimensions as <span class="math notranslate nohighlight">\(y\)</span>). Thus by squaring it we mean the dot product with itself, or equivalently, <span class="math notranslate nohighlight">\((y-X m)^2 = (y-X m)^T(y-X m)\)</span> where the <span class="math notranslate nohighlight">\(T\)</span> indicates vector transpose. Using the properties of transpose and multiplying everything out we get</p>
<div class="math notranslate nohighlight">
\[N\cdot L(m, b) = y^T y - y^T X m - m^T X^T y + m^T X^T X m\]</div>
<p>We now can take the partial derivative of both sides with respect to <span class="math notranslate nohighlight">\(m\)</span> and find the critical points. Doing so will give the closed form of the values of <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(b\)</span> which produce the line of best fit.</p>
<p>The point of the above math is to show that, if you define the loss function in the way we did (mean squared error), then we can just do some linear algebra and solve for what exactly the coefficients should be. Having said that, that’s generally <em>not</em> how we’ll find parameters for our models. Instead, we’ll use something called “gradient descent”. We’ll cover gradient descent soon, so let’s leave it there for now.</p>
</div>
<div class="section" id="linear-regression-using-scikit-learn-a-id-lr-sklearn-a">
<h2>Linear regression using scikit-learn <a id="lr_sklearn"></a><a class="headerlink" href="#linear-regression-using-scikit-learn-a-id-lr-sklearn-a" title="Permalink to this headline">¶</a></h2>
<p>We’ll now turn to actually <em>doing</em> linear regression by writing code. While we <em>could</em> write everything by hand, there are plenty of Python libraries that already exist to help you. We will be using the most popular (and extremely powerful) library called scikit-learn, or sklearn for short. Sklearn is used to build machine learning models. It comes with <a class="reference external" href="https://scikit-learn.org/stable/">many, many models</a> ready to go. Even though you won’t understand all of it, I highly encourage you to poke around their site and see what they have. It is a truly impressive package which is used across all industries. If you work as a data scientist, or any position which writes or uses machine learning models, you almost certainly will be using sklearn.</p>
<p>For this lesson we will be using their <code class="docutils literal notranslate"><span class="pre">linear_model</span></code> package, which contains linear regression (among other linear models). We have imported it at the top of this notebook using <code class="docutils literal notranslate"><span class="pre">from</span> <span class="pre">sklearn.linear_model</span> <span class="pre">import</span> <span class="pre">LinearRegression</span></code>.</p>
<p>Being able to read and understand the sklearn documentation is essential. Therefore, we will now take some time to familiarize ourself with it. <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression">Here is the link</a> to the documentation on <code class="docutils literal notranslate"><span class="pre">linear_model.LinearRegression</span></code>. I have also pasted a screenshot of (part of) it below.</p>
<p><img alt="Sklearn LinearRegression" src="https://drive.google.com/uc?id=12F9X2p5--NITMLTsopvXXbV5bfKz5IHE" /></p>
<p>At the very top is a short code snippet showing how to call this function. They say to call it using the command <code class="docutils literal notranslate"><span class="pre">sklearn.linear_model.LinearRegression(...)</span></code>. Rather than directly referencing <code class="docutils literal notranslate"><span class="pre">sklearn.linear_model</span></code> each time, we simply import <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn.linear_model</span></code>. That way we can just type <code class="docutils literal notranslate"><span class="pre">LinearRegression(...)</span></code>, and Python will know we mean the <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> function coming from <code class="docutils literal notranslate"><span class="pre">sklearn.linear_model</span></code>. If you need more reference on this, see homework 1 where we cover importing from packages.</p>
<p>Next is a brief description of what the function does.</p>
<p>Next are a list of the parameters, along with a description of each. Parameters are organized as follows:
<strong>name: data_type, (optional), default_value</strong>
That is, first is the name of the parameter. Next is the data type. If the parameter is optional (meaning you aren’t required to specify it when you call this function) then that is stated. Finally, for any required parameters (i.e. ones that aren’t optional) the default value is specified. What this means is that if you <em>do not</em> specify what value, then the default value will be used. Often the default values are a good starting point, and you should use them unless you have a good reason to do otherwise.</p>
<p>Let’s go through each parameter for using <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code>.</p>
<ul class="simple">
<li><p><strong>fit_intercept:</strong> As you know, a line has both a slope and y-intercept. LinearRegression always computes the best slope, but it’s up to you whether or not to compute the best y-intercept as well. Generally there’s no reason <em>not to</em> have it compute the y-intercept. That’s why this parameter defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>. Therefore, we don’t need to specify it.</p></li>
<li><p><strong>normalize:</strong> This parameter gives you the option to normalize your data before fitting a line to it. As you can see in the description, this means subtracting the mean and dividing by the standard deviation (this should sound familiar from MATH 3320…). We have no reason to do this, so we’ll keep the default value of <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>copy_X:</strong> Copies X. I have no idea why this matters and have never used it. We’ll leave the default value of <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>n_jobs:</strong> By “jobs” this is referring to parallel processing. We won’t worry about this for now, and will leave it as the default value of <code class="docutils literal notranslate"><span class="pre">None</span></code> (meaning 1).</p></li>
</ul>
<p>So as you can see, we’re not actually specifying any parameters at all. That means we can just <strong>instantiate</strong> (meaning call) the function, as down below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Note that we called it with no parameters, so the default values will be used. We also saved it to a variable we called <code class="docutils literal notranslate"><span class="pre">lr</span></code> (for “<strong>l</strong>inear <strong>r</strong>egression”). This is because we will now use this <strong>instance</strong> of the <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> function to, you guessed it, do linear regression!</p>
<p>So how do we actually <em>do</em> linear regression? The answer is that we use one of the <strong>methods</strong> (i.e. functions) from <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code>. If you scroll further down the sklearn documentation page you will see a section called “Methods”. Below is a screenshot of it.</p>
<p><img alt="Sklearn methods" src="https://drive.google.com/uc?id=1MTcFMZJxWXCOrr2bLlTLXOH-zYHtBE-S" /></p>
<p>First is a collection of small usage examples. You can see that they imported <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> just like we did. Next they made some fake data using <code class="docutils literal notranslate"><span class="pre">numpy</span></code> which they saved to <code class="docutils literal notranslate"><span class="pre">X</span></code>, and more fake data for <code class="docutils literal notranslate"><span class="pre">y</span></code>. Next they created a <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> object just like we did using the default values. However, they also combined one more step, namely using <code class="docutils literal notranslate"><span class="pre">.fit(...)</span></code>, which we will do next.</p>
<p>If you look down at the “Methods” section you will see <strong>fit</strong>. <strong>Nearly every sklearn model has a <code class="docutils literal notranslate"><span class="pre">.fit(...)</span></code> method</strong>. This method is used to “train” your model based on the data you give it. That is, in the case of linear regression, the model will determine the best possible slope and y-intercept based on the data you train it on. If you look at the fit method you will see the same setup as above, where it lists the parameters. Let’s go through them.</p>
<ul class="simple">
<li><p><strong>X:</strong> This is the input data. In our home price example, this would be the features we are using to predict home price, such as crime rate, pupil-teacher ratio, etc. We see that it should be a matrix (array) with shape (<code class="docutils literal notranslate"><span class="pre">n_samples</span></code>, <code class="docutils literal notranslate"><span class="pre">n_features</span></code>). A <strong>sample</strong> is simply an observation in your data, which is usually represented as a row. A <strong>feature</strong> is simply a variable in your data, usually represented as a column. So sklearn wants your X to be written as a matrix with samples as rows and features as columns. That is exactly what we have (not an accident!).</p></li>
<li><p><strong>y:</strong> This is what we want to predict, so median home value in our pricing example. The formatting gives us two options. First, we can have the number of samples (rows) first, and then nothing for columns. In Pandas/numpy, having nothing just means its one column. The alternative is to format it with observations as rows, and the value to want to predict in the columns.</p></li>
<li><p><strong>sample_weight:</strong> The weight (numerical importance) to assign to each sample. We have no reason to make any sample (row of our data) any more or less important than any other, so we will leave the default value of <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
</ul>
<p>Let’s now form our <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> parameters and then fit our model! We will use three columns from our data as the input, and the home value as the output.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;crime_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;avg_num_rooms&#39;</span><span class="p">,</span> <span class="s1">&#39;pct_below_poverty_line&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;median_home_value&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s check if the shapes match what sklearn wants. Remember that <code class="docutils literal notranslate"><span class="pre">X</span></code> should have <code class="docutils literal notranslate"><span class="pre">(n_samples,</span> <span class="pre">n_features)</span></code>, which is another way of saying <code class="docutils literal notranslate"><span class="pre">(number</span> <span class="pre">of</span> <span class="pre">rows,</span> <span class="pre">number</span> <span class="pre">of</span> <span class="pre">columns)</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(506, 3)
</pre></div>
</div>
</div>
</div>
<p>That’s perfect. We have 506 rows and 3 columns. Now let’s check <code class="docutils literal notranslate"><span class="pre">y</span></code>. Remember that <code class="docutils literal notranslate"><span class="pre">y</span></code> can either be of shape <code class="docutils literal notranslate"><span class="pre">(n_samples,</span> <span class="pre">)</span></code> or <code class="docutils literal notranslate"><span class="pre">(n_samples,</span> <span class="pre">n_targets)</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(506,)
</pre></div>
</div>
</div>
</div>
<p>As mentioned above, the blank value after the comma means 1 to Pandas/numpy. This matches the first option for the shape of <code class="docutils literal notranslate"><span class="pre">y</span></code>, so we’re good to go! Let’s go ahead and fit it. We will use the variable <code class="docutils literal notranslate"><span class="pre">lr</span></code> that we saved earlier, which is just the <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> object.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)
</pre></div>
</div>
</div>
</div>
<p>We can see that it printed out some basic information about the parameters of the <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> object, but not much more. So what did it do? It used the data we gave it (<code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code>) to find the line of best fit. How can you see this line? By accessing its <strong>attributes</strong>. Attributes are just constant. They are numbers/text/whatever that the object is storing. On the sklearn documentation page the attributes are listed. Here is a screenshot.</p>
<p><img alt="Sklearn attributes" src="https://drive.google.com/uc?id=1tNZK1_K0D7AcZ3C-3swIl_HKfWDt0enh" /></p>
<p>The <strong>coef_</strong> attribute holds the coefficients, in other words, the slopes. Remember from our earlier mathematical work that the line has three slopes because we have three variables (<code class="docutils literal notranslate"><span class="pre">'crime_rate',</span> <span class="pre">'avg_num_rooms',</span> <span class="pre">'pct_below_poverty_line'</span></code>). At the bottom the <strong>intercept_</strong> is the y-intercept. Let’s look at all of these. You access an attribute in exactly the same way as you accessed the <code class="docutils literal notranslate"><span class="pre">.fit(...)</span></code> method. However, since it’s a <em>constant</em> and not a <em>function</em>, we don’t use the parentheses. To makea sense of this, just think about how you write functions in math. You know that sine is a <em>function</em>, so you write sin(…) (for example, sin(pi/2)). However, you know that pi is a <em>constant</em>, so you write pi, not pi(). It’s the exact same idea here.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">coef_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-102.94088672, 5216.95492439, -578.48581963])
</pre></div>
</div>
</div>
</div>
<p>So the slope for the first variable (crime rate) is -102.94, for the second variable (average number of rooms) the coefficient is 5216.95, and for percent below poverty line the coefficient is -578.48. We’ll return to these in a second, but first let’s look at the y-intercept.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">intercept_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-2562.251011928376
</pre></div>
</div>
</div>
</div>
<p>So that means the line has the following formula:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;median_home_value = </span><span class="si">{</span><span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">crime_rate + </span><span class="si">{</span><span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">avg_num_rooms + </span><span class="si">{</span><span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">pct_below_poverty_line + </span><span class="si">{</span><span class="n">lr</span><span class="o">.</span><span class="n">intercept_</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>median_home_value = -102.94crime_rate + 5216.95avg_num_rooms + -578.49pct_below_poverty_line + -2562.25
</pre></div>
</div>
</div>
</div>
<p>So what do we learn from these coefficients? Just like any slope, they tell us how much the y values are changing for every unit change in the x. So for instance, the first coefficient of -102.94 means that for every 1% the crime rate increases (the x value), the median home value (the y value) decreases by \<span class="math notranslate nohighlight">\(102.94. Similarly, for every extra room (the x value), the median home value (the y value) increases by \\\)</span>5,216.95. Finally, for every 1% the poverty level increases, the median home value drops by \<span class="math notranslate nohighlight">\(578.49. The y-intercept can be interpreted as it always is: if all the x values are zero (zero crime, zero rooms, zero poverty), then the predicted home value is -\\\)</span>2,562.25. Obviously this is meaningless, and thus y-intercept (and all values returned by any model!) should always be interpreted using logic. Don’t just trust the numbers, use your brain!</p>
<p>It is interesting to see the equation, but to actually <em>use</em> the regression line, we will continue to work with the <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> object that we saved in <code class="docutils literal notranslate"><span class="pre">lr</span></code>. In particular, back in the methods section of the sklearn documentation you will see a function called <code class="docutils literal notranslate"><span class="pre">.predict()</span></code>. Below is a screenshot.</p>
<p><img alt="Sklearn attributes" src="https://drive.google.com/uc?id=1Tl_7Y3pnA5dvCraPt61PhNBw3TeNo9PY" /></p>
<p>As you can see, <strong>predict</strong> simple takes in an X value of samples (observations) and then predicts what the y values will be. Let’s take the same X values as we used to train our model and see what the predicted home prices are.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We actually defined this above and don&#39;t need to do it again. But I&#39;m leaving them here for clarity.</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;crime_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;avg_num_rooms&#39;</span><span class="p">,</span> <span class="s1">&#39;pct_below_poverty_line&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;median_home_value&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Big output coming...</span>
<span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([28857.71764778, 25645.64485054, 32587.46300992, 32241.91904276,
       31632.88834584, 27965.78524617, 21602.4146046 , 18543.91123028,
        9478.59635279, 18850.73477002, 18853.08515341, 21097.83183626,
       19062.73051503, 23630.2929103 , 23239.36952965, 22909.10504177,
       24485.44728242, 20120.1957678 , 19056.32414487, 20715.21529903,
       14207.55561567, 20468.71647343, 18524.16003551, 16161.8600086 ,
       18836.43867049, 17010.15279468, 19127.36596361, 18890.05264132,
       23837.68045468, 25222.25294238, 14052.02636331, 21432.18812292,
       12305.90780482, 16445.83514372, 17307.87951492, 22783.59410375,
       21299.42893944, 22875.36497493, 22684.0209119 , 29341.66371693,
       32932.78066976, 29943.54367259, 26245.57516912, 25519.9168657 ,
       23562.27902933, 21156.50052322, 19418.08588738, 17996.85251596,
        7754.80691906, 17269.03687779, 20756.68298329, 23879.84252534,
       28345.41974161, 23847.28167951, 19592.18945627, 32471.58888754,
       27397.59365597, 30710.02065169, 25511.62505583, 23014.36983315,
       19765.82754763, 20191.09085184, 27213.84912225, 27206.16087814,
       29840.17754622, 27547.17728344, 21700.06456106, 23411.3156467 ,
       19035.06284834, 23041.44545951, 27018.43846731, 22804.23449923,
       25875.90537729, 25635.73579102, 26233.44515927, 25050.07267815,
       23260.0821091 , 23519.83979321, 22805.48502015, 22809.28760313,
       29467.78081994, 27787.51279252, 26423.80760662, 25262.62607048,
       25198.63263569, 28242.74407474, 21373.06464461, 24480.94845977,
       30805.45368113, 31065.74580013, 25813.6669158 , 26104.71392216,
       26320.60171734, 26244.89953307, 23907.91519902, 28140.58175581,
       23017.97067286, 37085.49462501, 36160.71435035, 32538.79773902,
       27067.5651513 , 28365.16589163, 24679.49225887, 21657.56862217,
       22463.60930294, 18420.83878094, 17071.72905267, 21237.66385509,
       24101.11566938, 20911.56841086, 22225.35867955, 26581.80481889,
       18895.65673783, 19310.25050019, 24004.75875247, 19229.28337093,
       22679.14837087, 22875.08863055, 19166.93923536, 19448.0239318 ,
       19741.33127436, 20497.97799343, 18154.1940733 , 13273.43209789,
       17928.3076315 , 20081.66710805, 10911.11938302, 17167.02500235,
       22051.58928993, 16145.57730838, 23804.91648449, 23225.16738968,
       24186.68318178, 19082.26672555, 17357.68114457, 20618.62056533,
       18627.27194932, 22631.59879904, 15634.42167216, 18792.33712595,
       15641.06578499,  3548.29505427,  9768.09493017, 10258.6731707 ,
        5786.48042487, 13090.84551939, 16945.13964774,  5809.95484876,
        7870.20463643, 13947.08757271, 21048.76510544, 17793.84920715,
       16457.90779102, 17865.81506451, 20520.21715414, 20479.70160809,
       15352.91081188, 30877.86070585, 25225.90059888, 26978.4225164 ,
       26730.95415595, 35356.10437291, 36840.96862466, 39052.80429295,
       21013.3976482 , 23290.44593843, 36455.65630988, 20889.65280884,
       23745.68934158, 24036.07814555, 19615.51590613, 20916.06461554,
       17994.3419414 , 25670.77205891, 22418.58917213, 28497.74652382,
       22988.09748736, 26738.55898702, 29216.10056386, 30930.57584614,
       33567.26943395, 24016.93847974, 31967.38911256, 28380.52237741,
       18577.78029703, 21924.35927427, 35711.69435516, 28946.74541533,
       28989.26120073, 31794.9153998 , 30741.1903078 , 29874.5954848 ,
       33215.8783346 , 30001.007484  , 29355.27009942, 36801.74451972,
       33089.34841906, 29529.08150692, 31552.42416953, 31184.87161026,
       32084.63018729, 25282.92927876, 35337.44404055, 36198.85181141,
       37682.6576077 , 21868.6463925 , 24070.57190614, 17134.00533922,
       20578.77495549, 11915.06615494, 18522.39116529, 11719.40031828,
       18437.12081412, 25255.17438952,  8547.84653497, 24190.31926436,
       20335.14205522, 26476.01675926, 18105.98732856, 24599.54061878,
       28046.84325677, 17138.78342378, 27516.6279047 , 27503.78668543,
       38133.70675002, 40223.04871707, 37532.0681343 , 31085.17653577,
       35236.90418099, 29398.65374595, 21845.71733598, 33021.11082169,
       39443.42135553, 38142.83544628, 27824.06870893, 22860.19412431,
       26459.03340562, 33035.17716085, 27561.1775935 , 27627.98863708,
       26824.25630113, 22050.9410019 , 24105.94488384, 27774.26824691,
       19363.89107685, 15980.23742901, 23968.99715689, 24026.64418797,
       25474.31256142, 28670.75127803, 27852.62664037, 28925.7020844 ,
       31681.58668348, 38438.76089639, 25497.29710995, 22737.92994954,
       34524.25686461, 39821.32478361, 31118.97260596, 29073.00629204,
       29412.19689587, 32414.46018256, 37777.34157628, 29069.45925206,
       30288.76634067, 20320.43971439, 25392.7849007 , 36359.35466527,
       34524.74753398, 20416.45911111, 20437.12557442, 26162.63841776,
       27062.70787238, 33732.07209445, 30646.06544017, 31460.9838546 ,
       31838.74433343, 30635.87510418, 27086.52514089, 30148.33300407,
       36055.54659009, 31130.42702928, 35573.81782102, 36942.1225213 ,
       29873.47916362, 26340.69258749, 22457.53373076, 25695.44851973,
       25981.60211971, 26181.2345521 , 31301.31395511, 32661.01535142,
       29303.55547147, 24430.11541309, 22761.73848127, 28636.1555686 ,
       27323.0465964 , 18466.18486407, 27657.59730204, 31422.58128296,
       29767.49044944, 26318.22563415, 26296.86066323, 31040.7930981 ,
       31173.05090103, 26781.60426735, 32397.02761932, 28807.59799887,
       29375.17149196, 22789.93262899, 15798.30641879, 25835.23632753,
       22052.67609095, 25529.42035316, 26291.13779499, 20521.78821472,
       17654.41234104, 18355.87523172, 24697.85792887, 21915.92465355,
       26779.55053825, 26708.16296951, 24462.8862122 , 20395.47719746,
       27329.06950485, 28029.52790459, 26778.24120921, 21748.84031812,
       22276.5252642 , 26223.71686293, 24227.60630919, 20009.94432788,
       24368.09223823, 27097.00434976, 26448.10734813, 24294.75696285,
       22383.36862274, 22079.75757968, 24120.96132418, 23021.1136668 ,
       23192.07081852, 32036.49318489, 26550.16039043, 28214.3331901 ,
       30629.13884221, 22717.85567283, 20871.59057995, 27750.33246797,
       28585.56970911, 30227.9334745 , 27830.04758751, 28576.02448538,
       23620.45019043, 29932.47627129, 22320.12638933, 25172.4549348 ,
       18739.4086851 , 22727.37737144, 22225.53978414, 21555.69912123,
       25841.8727872 , 21445.25272806, 19137.65156226, 18808.04944423,
       39824.77777448, 11427.52114048, 14850.12749182,  8487.64262691,
       20975.92408016, 29561.23387169, 31654.48095907, 23402.98978855,
       22099.36704482,  1774.70023474, -4843.81921696, 25795.91279255,
       17107.51155594, 19583.46316412, 14583.2256613 , 15464.22439023,
       14674.46102055, 17754.11307824, 11720.82612107, 11204.97546225,
         438.83002681,  5413.85042186,  2836.32491469,  2690.59672871,
        3707.72943399, 12656.5593473 , 16627.63020274, 17608.27799881,
        7662.9788468 , 20080.82169114, 17316.4498186 , 20395.6770974 ,
       19042.60091866, 15106.38686975,  4241.95466897,  9609.33074103,
       10607.324986  , 17308.57516705, 18110.29043105, 11353.46723438,
        6178.73226064,  6800.27230962,  3391.10543754, 18447.14238066,
       10707.19615831, 20255.58321297, 16359.30225627, 18445.15731013,
        -237.16226269,  9765.30290346, -5088.38141654, 12336.97915133,
       16784.70584208,  7027.26730602,  9017.10641382, 18667.51321749,
       21053.44321007, 18965.64125408, 17506.20795286, 15078.11366585,
       15638.21784848, 12454.97273582, 17551.9347439 , 17516.7399466 ,
       16538.95916038, 15830.62925137, 19476.30159747, 20658.99411345,
       23334.14374606, 21056.51059238, 19616.71754161, 17410.71668688,
       19218.33577101, 12669.15132774,  7312.2531378 , 12591.11901538,
       12729.71147899, 18564.50332269, 19701.61076289, 19339.27950756,
       12897.74535236, 16163.40633504, 19579.89793083, 19516.95448614,
       18256.32248862, 18969.65341794, 21866.65702488, 21339.91465146,
       19774.47742857, 25473.77842512, 20734.61052594, 20501.21877798,
       17136.64268488, 17761.86865788, 20122.93568378, 19958.19186418,
       22402.46184885, 21846.06431023, 21615.17326296, 24863.9229839 ,
       21375.25302601, 18982.52397701, 18179.43310281, 15966.57830011,
       16262.09959082, 17357.79938542, 19739.50770623, 22073.33120711,
       22344.86838285, 26628.59472633, 14427.16042879, 14985.00353733,
       19956.90234212,  9151.87402247, 18221.12177367, 20874.68832878,
       23189.51516962, 27587.10937302, 29629.51142126, 21179.89448554,
       20104.63841036, 23868.61765182, 20082.58002573, 21122.41702414,
       15428.03758097, 11797.16158173,  6816.88517493, 18186.66657058,
       20916.54522911, 20245.45536271, 20463.02239751, 16818.10759452,
       13298.13412271, 19480.508488  , 21339.9486635 , 17737.52910767,
       20567.53664589, 26232.72774101, 24108.20174915, 30562.31182964,
       29121.87134141, 24332.63849602])
</pre></div>
</div>
</div>
</div>
<p>As you can see, this returns many, many predictions. Let’s save those.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Notice that this is exactly the <span class="math notranslate nohighlight">\(R^2\)</span> value we got using <code class="docutils literal notranslate"><span class="pre">np.corrcoef(...)**2</span></code>! So sklearn is doing that work for us. It doesn’t make a scatterplot, but it does at least compute the correlation coefficient.</p>
<div class="section" id="linear-regression-with-just-one-variable">
<h3>Linear regression with just one variable<a class="headerlink" href="#linear-regression-with-just-one-variable" title="Permalink to this headline">¶</a></h3>
<p>We have just one final topic to cover. Up to this point we’ve been using multiple variables to predict home price. But what if we wanted to use just a single variable, like <code class="docutils literal notranslate"><span class="pre">pct_below_poverty_line</span></code>. Let’s try doing the same thing.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr_single</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>

<span class="n">X_single</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;pct_below_poverty_line&#39;</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;median_home_value&#39;</span><span class="p">]</span> <span class="c1"># y is the same regardless, it&#39;s just the home prices</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>An error is coming!</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr_single</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_single</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ValueError</span><span class="g g-Whitespace">                                </span>Traceback (most recent call last)
<span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="mi">136</span><span class="o">-</span><span class="mi">13</span><span class="n">adedf82583</span><span class="o">&gt;</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">lr_single</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_single</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nn">~/anaconda3/envs/math_3439/lib/python3.8/site-packages/sklearn/linear_model/_base.py</span> in <span class="ni">fit</span><span class="nt">(self, X, y, sample_weight)</span>
<span class="g g-Whitespace">    </span><span class="mi">489</span> 
<span class="g g-Whitespace">    </span><span class="mi">490</span>         <span class="n">n_jobs_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span>
<span class="ne">--&gt; </span><span class="mi">491</span>         <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">check_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="s1">&#39;csc&#39;</span><span class="p">,</span> <span class="s1">&#39;coo&#39;</span><span class="p">],</span>
<span class="g g-Whitespace">    </span><span class="mi">492</span>                          <span class="n">y_numeric</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">multi_output</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">493</span> 

<span class="nn">~/anaconda3/envs/math_3439/lib/python3.8/site-packages/sklearn/utils/validation.py</span> in <span class="ni">check_X_y</span><span class="nt">(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)</span>
<span class="g g-Whitespace">    </span><span class="mi">745</span>         <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;y cannot be None&quot;</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">746</span> 
<span class="ne">--&gt; </span><span class="mi">747</span>     <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="n">accept_sparse</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">748</span>                     <span class="n">accept_large_sparse</span><span class="o">=</span><span class="n">accept_large_sparse</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">749</span>                     <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="n">order</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">,</span>

<span class="nn">~/anaconda3/envs/math_3439/lib/python3.8/site-packages/sklearn/utils/validation.py</span> in <span class="ni">check_array</span><span class="nt">(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)</span>
<span class="g g-Whitespace">    </span><span class="mi">550</span>             <span class="c1"># If input is 1D raise error</span>
<span class="g g-Whitespace">    </span><span class="mi">551</span>             <span class="k">if</span> <span class="n">array</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">552</span>                 <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">553</span>                     <span class="s2">&quot;Expected 2D array, got 1D array instead:</span><span class="se">\n</span><span class="s2">array=</span><span class="si">{}</span><span class="s2">.</span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">554</span>                     <span class="s2">&quot;Reshape your data either using array.reshape(-1, 1) if &quot;</span>

<span class="ne">ValueError</span>: Expected 2D array, got 1D array instead:
<span class="n">array</span><span class="o">=</span><span class="p">[</span> <span class="mf">4.98</span>  <span class="mf">9.14</span>  <span class="mf">4.03</span>  <span class="mf">2.94</span>  <span class="mf">5.33</span>  <span class="mf">5.21</span> <span class="mf">12.43</span> <span class="mf">19.15</span> <span class="mf">29.93</span> <span class="mf">17.1</span>  <span class="mf">20.45</span> <span class="mf">13.27</span>
<span class="g g-Whitespace"> </span><span class="mi">15</span><span class="o">.</span><span class="mi">71</span>  <span class="mf">8.26</span> <span class="mf">10.26</span>  <span class="mf">8.47</span>  <span class="mf">6.58</span> <span class="mf">14.67</span> <span class="mf">11.69</span> <span class="mf">11.28</span> <span class="mf">21.02</span> <span class="mf">13.83</span> <span class="mf">18.72</span> <span class="mf">19.88</span>
<span class="g g-Whitespace"> </span><span class="mi">16</span><span class="o">.</span><span class="mi">3</span>  <span class="mf">16.51</span> <span class="mf">14.81</span> <span class="mf">17.28</span> <span class="mf">12.8</span>  <span class="mf">11.98</span> <span class="mf">22.6</span>  <span class="mf">13.04</span> <span class="mf">27.71</span> <span class="mf">18.35</span> <span class="mf">20.34</span>  <span class="mf">9.68</span>
<span class="g g-Whitespace"> </span><span class="mi">11</span><span class="o">.</span><span class="mi">41</span>  <span class="mf">8.77</span> <span class="mf">10.13</span>  <span class="mf">4.32</span>  <span class="mf">1.98</span>  <span class="mf">4.84</span>  <span class="mf">5.81</span>  <span class="mf">7.44</span>  <span class="mf">9.55</span> <span class="mf">10.21</span> <span class="mf">14.15</span> <span class="mf">18.8</span>
<span class="g g-Whitespace"> </span><span class="mi">30</span><span class="o">.</span><span class="mi">81</span> <span class="mf">16.2</span>  <span class="mf">13.45</span>  <span class="mf">9.43</span>  <span class="mf">5.28</span>  <span class="mf">8.43</span> <span class="mf">14.8</span>   <span class="mf">4.81</span>  <span class="mf">5.77</span>  <span class="mf">3.95</span>  <span class="mf">6.86</span>  <span class="mf">9.22</span>
<span class="g g-Whitespace"> </span><span class="mi">13</span><span class="o">.</span><span class="mi">15</span> <span class="mf">14.44</span>  <span class="mf">6.73</span>  <span class="mf">9.5</span>   <span class="mf">8.05</span>  <span class="mf">4.67</span> <span class="mf">10.24</span>  <span class="mf">8.1</span>  <span class="mf">13.09</span>  <span class="mf">8.79</span>  <span class="mf">6.72</span>  <span class="mf">9.88</span>
<span class="g g-Whitespace">  </span><span class="mi">5</span><span class="o">.</span><span class="mi">52</span>  <span class="mf">7.54</span>  <span class="mf">6.78</span>  <span class="mf">8.94</span> <span class="mf">11.97</span> <span class="mf">10.27</span> <span class="mf">12.34</span>  <span class="mf">9.1</span>   <span class="mf">5.29</span>  <span class="mf">7.22</span>  <span class="mf">6.72</span>  <span class="mf">7.51</span>
<span class="g g-Whitespace">  </span><span class="mi">9</span><span class="o">.</span><span class="mi">62</span>  <span class="mf">6.53</span> <span class="mf">12.86</span>  <span class="mf">8.44</span>  <span class="mf">5.5</span>   <span class="mf">5.7</span>   <span class="mf">8.81</span>  <span class="mf">8.2</span>   <span class="mf">8.16</span>  <span class="mf">6.21</span> <span class="mf">10.59</span>  <span class="mf">6.65</span>
<span class="g g-Whitespace"> </span><span class="mi">11</span><span class="o">.</span><span class="mi">34</span>  <span class="mf">4.21</span>  <span class="mf">3.57</span>  <span class="mf">6.19</span>  <span class="mf">9.42</span>  <span class="mf">7.67</span> <span class="mf">10.63</span> <span class="mf">13.44</span> <span class="mf">12.33</span> <span class="mf">16.47</span> <span class="mf">18.66</span> <span class="mf">14.09</span>
<span class="g g-Whitespace"> </span><span class="mi">12</span><span class="o">.</span><span class="mi">27</span> <span class="mf">15.55</span> <span class="mf">13.</span>   <span class="mf">10.16</span> <span class="mf">16.21</span> <span class="mf">17.09</span> <span class="mf">10.45</span> <span class="mf">15.76</span> <span class="mf">12.04</span> <span class="mf">10.3</span>  <span class="mf">15.37</span> <span class="mf">13.61</span>
<span class="g g-Whitespace"> </span><span class="mi">14</span><span class="o">.</span><span class="mi">37</span> <span class="mf">14.27</span> <span class="mf">17.93</span> <span class="mf">25.41</span> <span class="mf">17.58</span> <span class="mf">14.81</span> <span class="mf">27.26</span> <span class="mf">17.19</span> <span class="mf">15.39</span> <span class="mf">18.34</span> <span class="mf">12.6</span>  <span class="mf">12.26</span>
<span class="g g-Whitespace"> </span><span class="mi">11</span><span class="o">.</span><span class="mi">12</span> <span class="mf">15.03</span> <span class="mf">17.31</span> <span class="mf">16.96</span> <span class="mf">16.9</span>  <span class="mf">14.59</span> <span class="mf">21.32</span> <span class="mf">18.46</span> <span class="mf">24.16</span> <span class="mf">34.41</span> <span class="mf">26.82</span> <span class="mf">26.42</span>
<span class="g g-Whitespace"> </span><span class="mi">29</span><span class="o">.</span><span class="mi">29</span> <span class="mf">27.8</span>  <span class="mf">16.65</span> <span class="mf">29.53</span> <span class="mf">28.32</span> <span class="mf">21.45</span> <span class="mf">14.1</span>  <span class="mf">13.28</span> <span class="mf">12.12</span> <span class="mf">15.79</span> <span class="mf">15.12</span> <span class="mf">15.02</span>
<span class="g g-Whitespace"> </span><span class="mi">16</span><span class="o">.</span><span class="mi">14</span>  <span class="mf">4.59</span>  <span class="mf">6.43</span>  <span class="mf">7.39</span>  <span class="mf">5.5</span>   <span class="mf">1.73</span>  <span class="mf">1.92</span>  <span class="mf">3.32</span> <span class="mf">11.64</span>  <span class="mf">9.81</span>  <span class="mf">3.7</span>  <span class="mf">12.14</span>
<span class="g g-Whitespace"> </span><span class="mi">11</span><span class="o">.</span><span class="mi">1</span>  <span class="mf">11.32</span> <span class="mf">14.43</span> <span class="mf">12.03</span> <span class="mf">14.69</span>  <span class="mf">9.04</span>  <span class="mf">9.64</span>  <span class="mf">5.33</span> <span class="mf">10.11</span>  <span class="mf">6.29</span>  <span class="mf">6.92</span>  <span class="mf">5.04</span>
<span class="g g-Whitespace">  </span><span class="mi">7</span><span class="o">.</span><span class="mi">56</span>  <span class="mf">9.45</span>  <span class="mf">4.82</span>  <span class="mf">5.68</span> <span class="mf">13.98</span> <span class="mf">13.15</span>  <span class="mf">4.45</span>  <span class="mf">6.68</span>  <span class="mf">4.56</span>  <span class="mf">5.39</span>  <span class="mf">5.1</span>   <span class="mf">4.69</span>
<span class="g g-Whitespace">  </span><span class="mi">2</span><span class="o">.</span><span class="mi">87</span>  <span class="mf">5.03</span>  <span class="mf">4.38</span>  <span class="mf">2.97</span>  <span class="mf">4.08</span>  <span class="mf">8.61</span>  <span class="mf">6.62</span>  <span class="mf">4.56</span>  <span class="mf">4.45</span>  <span class="mf">7.43</span>  <span class="mf">3.11</span>  <span class="mf">3.81</span>
<span class="g g-Whitespace">  </span><span class="mi">2</span><span class="o">.</span><span class="mi">88</span> <span class="mf">10.87</span> <span class="mf">10.97</span> <span class="mf">18.06</span> <span class="mf">14.66</span> <span class="mf">23.09</span> <span class="mf">17.27</span> <span class="mf">23.98</span> <span class="mf">16.03</span>  <span class="mf">9.38</span> <span class="mf">29.55</span>  <span class="mf">9.47</span>
<span class="g g-Whitespace"> </span><span class="mi">13</span><span class="o">.</span><span class="mi">51</span>  <span class="mf">9.69</span> <span class="mf">17.92</span> <span class="mf">10.5</span>   <span class="mf">9.71</span> <span class="mf">21.46</span>  <span class="mf">9.93</span>  <span class="mf">7.6</span>   <span class="mf">4.14</span>  <span class="mf">4.63</span>  <span class="mf">3.13</span>  <span class="mf">6.36</span>
<span class="g g-Whitespace">  </span><span class="mi">3</span><span class="o">.</span><span class="mi">92</span>  <span class="mf">3.76</span> <span class="mf">11.65</span>  <span class="mf">5.25</span>  <span class="mf">2.47</span>  <span class="mf">3.95</span>  <span class="mf">8.05</span> <span class="mf">10.88</span>  <span class="mf">9.54</span>  <span class="mf">4.73</span>  <span class="mf">6.36</span>  <span class="mf">7.37</span>
<span class="g g-Whitespace"> </span><span class="mi">11</span><span class="o">.</span><span class="mi">38</span> <span class="mf">12.4</span>  <span class="mf">11.22</span>  <span class="mf">5.19</span> <span class="mf">12.5</span>  <span class="mf">18.46</span>  <span class="mf">9.16</span> <span class="mf">10.15</span>  <span class="mf">9.52</span>  <span class="mf">6.56</span>  <span class="mf">5.9</span>   <span class="mf">3.59</span>
<span class="g g-Whitespace">  </span><span class="mi">3</span><span class="o">.</span><span class="mi">53</span>  <span class="mf">3.54</span>  <span class="mf">6.57</span>  <span class="mf">9.25</span>  <span class="mf">3.11</span>  <span class="mf">5.12</span>  <span class="mf">7.79</span>  <span class="mf">6.9</span>   <span class="mf">9.59</span>  <span class="mf">7.26</span>  <span class="mf">5.91</span> <span class="mf">11.25</span>
<span class="g g-Whitespace">  </span><span class="mi">8</span><span class="o">.</span><span class="mi">1</span>  <span class="mf">10.45</span> <span class="mf">14.79</span>  <span class="mf">7.44</span>  <span class="mf">3.16</span> <span class="mf">13.65</span> <span class="mf">13.</span>    <span class="mf">6.59</span>  <span class="mf">7.73</span>  <span class="mf">6.58</span>  <span class="mf">3.53</span>  <span class="mf">2.98</span>
<span class="g g-Whitespace">  </span><span class="mi">6</span><span class="o">.</span><span class="mi">05</span>  <span class="mf">4.16</span>  <span class="mf">7.19</span>  <span class="mf">4.85</span>  <span class="mf">3.76</span>  <span class="mf">4.59</span>  <span class="mf">3.01</span>  <span class="mf">3.16</span>  <span class="mf">7.85</span>  <span class="mf">8.23</span> <span class="mf">12.93</span>  <span class="mf">7.14</span>
<span class="g g-Whitespace">  </span><span class="mi">7</span><span class="o">.</span><span class="mi">6</span>   <span class="mf">9.51</span>  <span class="mf">3.33</span>  <span class="mf">3.56</span>  <span class="mf">4.7</span>   <span class="mf">8.58</span> <span class="mf">10.4</span>   <span class="mf">6.27</span>  <span class="mf">7.39</span> <span class="mf">15.84</span>  <span class="mf">4.97</span>  <span class="mf">4.74</span>
<span class="g g-Whitespace">  </span><span class="mi">6</span><span class="o">.</span><span class="mi">07</span>  <span class="mf">9.5</span>   <span class="mf">8.67</span>  <span class="mf">4.86</span>  <span class="mf">6.93</span>  <span class="mf">8.93</span>  <span class="mf">6.47</span>  <span class="mf">7.53</span>  <span class="mf">4.54</span>  <span class="mf">9.97</span> <span class="mf">12.64</span>  <span class="mf">5.98</span>
<span class="g g-Whitespace"> </span><span class="mi">11</span><span class="o">.</span><span class="mi">72</span>  <span class="mf">7.9</span>   <span class="mf">9.28</span> <span class="mf">11.5</span>  <span class="mf">18.33</span> <span class="mf">15.94</span> <span class="mf">10.36</span> <span class="mf">12.73</span>  <span class="mf">7.2</span>   <span class="mf">6.87</span>  <span class="mf">7.7</span>  <span class="mf">11.74</span>
<span class="g g-Whitespace">  </span><span class="mi">6</span><span class="o">.</span><span class="mi">12</span>  <span class="mf">5.08</span>  <span class="mf">6.15</span> <span class="mf">12.79</span>  <span class="mf">9.97</span>  <span class="mf">7.34</span>  <span class="mf">9.09</span> <span class="mf">12.43</span>  <span class="mf">7.83</span>  <span class="mf">5.68</span>  <span class="mf">6.75</span>  <span class="mf">8.01</span>
<span class="g g-Whitespace">  </span><span class="mi">9</span><span class="o">.</span><span class="mi">8</span>  <span class="mf">10.56</span>  <span class="mf">8.51</span>  <span class="mf">9.74</span>  <span class="mf">9.29</span>  <span class="mf">5.49</span>  <span class="mf">8.65</span>  <span class="mf">7.18</span>  <span class="mf">4.61</span> <span class="mf">10.53</span> <span class="mf">12.67</span>  <span class="mf">6.36</span>
<span class="g g-Whitespace">  </span><span class="mi">5</span><span class="o">.</span><span class="mi">99</span>  <span class="mf">5.89</span>  <span class="mf">5.98</span>  <span class="mf">5.49</span>  <span class="mf">7.79</span>  <span class="mf">4.5</span>   <span class="mf">8.05</span>  <span class="mf">5.57</span> <span class="mf">17.6</span>  <span class="mf">13.27</span> <span class="mf">11.48</span> <span class="mf">12.67</span>
<span class="g g-Whitespace">  </span><span class="mi">7</span><span class="o">.</span><span class="mi">79</span> <span class="mf">14.19</span> <span class="mf">10.19</span> <span class="mf">14.64</span>  <span class="mf">5.29</span>  <span class="mf">7.12</span> <span class="mf">14.</span>   <span class="mf">13.33</span>  <span class="mf">3.26</span>  <span class="mf">3.73</span>  <span class="mf">2.96</span>  <span class="mf">9.53</span>
<span class="g g-Whitespace">  </span><span class="mi">8</span><span class="o">.</span><span class="mi">88</span> <span class="mf">34.77</span> <span class="mf">37.97</span> <span class="mf">13.44</span> <span class="mf">23.24</span> <span class="mf">21.24</span> <span class="mf">23.69</span> <span class="mf">21.78</span> <span class="mf">17.21</span> <span class="mf">21.08</span> <span class="mf">23.6</span>  <span class="mf">24.56</span>
<span class="g g-Whitespace"> </span><span class="mi">30</span><span class="o">.</span><span class="mi">63</span> <span class="mf">30.81</span> <span class="mf">28.28</span> <span class="mf">31.99</span> <span class="mf">30.62</span> <span class="mf">20.85</span> <span class="mf">17.11</span> <span class="mf">18.76</span> <span class="mf">25.68</span> <span class="mf">15.17</span> <span class="mf">16.35</span> <span class="mf">17.12</span>
<span class="g g-Whitespace"> </span><span class="mi">19</span><span class="o">.</span><span class="mi">37</span> <span class="mf">19.92</span> <span class="mf">30.59</span> <span class="mf">29.97</span> <span class="mf">26.77</span> <span class="mf">20.32</span> <span class="mf">20.31</span> <span class="mf">19.77</span> <span class="mf">27.38</span> <span class="mf">22.98</span> <span class="mf">23.34</span> <span class="mf">12.13</span>
<span class="g g-Whitespace"> </span><span class="mi">26</span><span class="o">.</span><span class="mi">4</span>  <span class="mf">19.78</span> <span class="mf">10.11</span> <span class="mf">21.22</span> <span class="mf">34.37</span> <span class="mf">20.08</span> <span class="mf">36.98</span> <span class="mf">29.05</span> <span class="mf">25.79</span> <span class="mf">26.64</span> <span class="mf">20.62</span> <span class="mf">22.74</span>
<span class="g g-Whitespace"> </span><span class="mi">15</span><span class="o">.</span><span class="mi">02</span> <span class="mf">15.7</span>  <span class="mf">14.1</span>  <span class="mf">23.29</span> <span class="mf">17.16</span> <span class="mf">24.39</span> <span class="mf">15.69</span> <span class="mf">14.52</span> <span class="mf">21.52</span> <span class="mf">24.08</span> <span class="mf">17.64</span> <span class="mf">19.69</span>
<span class="g g-Whitespace"> </span><span class="mi">12</span><span class="o">.</span><span class="mi">03</span> <span class="mf">16.22</span> <span class="mf">15.17</span> <span class="mf">23.27</span> <span class="mf">18.05</span> <span class="mf">26.45</span> <span class="mf">34.02</span> <span class="mf">22.88</span> <span class="mf">22.11</span> <span class="mf">19.52</span> <span class="mf">16.59</span> <span class="mf">18.85</span>
<span class="g g-Whitespace"> </span><span class="mi">23</span><span class="o">.</span><span class="mi">79</span> <span class="mf">23.98</span> <span class="mf">17.79</span> <span class="mf">16.44</span> <span class="mf">18.13</span> <span class="mf">19.31</span> <span class="mf">17.44</span> <span class="mf">17.73</span> <span class="mf">17.27</span> <span class="mf">16.74</span> <span class="mf">18.71</span> <span class="mf">18.13</span>
<span class="g g-Whitespace"> </span><span class="mi">19</span><span class="o">.</span><span class="mi">01</span> <span class="mf">16.94</span> <span class="mf">16.23</span> <span class="mf">14.7</span>  <span class="mf">16.42</span> <span class="mf">14.65</span> <span class="mf">13.99</span> <span class="mf">10.29</span> <span class="mf">13.22</span> <span class="mf">14.13</span> <span class="mf">17.15</span> <span class="mf">21.32</span>
<span class="g g-Whitespace"> </span><span class="mi">18</span><span class="o">.</span><span class="mi">13</span> <span class="mf">14.76</span> <span class="mf">16.29</span> <span class="mf">12.87</span> <span class="mf">14.36</span> <span class="mf">11.66</span> <span class="mf">18.14</span> <span class="mf">24.1</span>  <span class="mf">18.68</span> <span class="mf">24.91</span> <span class="mf">18.03</span> <span class="mf">13.11</span>
<span class="g g-Whitespace"> </span><span class="mi">10</span><span class="o">.</span><span class="mi">74</span>  <span class="mf">7.74</span>  <span class="mf">7.01</span> <span class="mf">10.42</span> <span class="mf">13.34</span> <span class="mf">10.58</span> <span class="mf">14.98</span> <span class="mf">11.45</span> <span class="mf">18.06</span> <span class="mf">23.97</span> <span class="mf">29.68</span> <span class="mf">18.07</span>
<span class="g g-Whitespace"> </span><span class="mi">13</span><span class="o">.</span><span class="mi">35</span> <span class="mf">12.01</span> <span class="mf">13.59</span> <span class="mf">17.6</span>  <span class="mf">21.14</span> <span class="mf">14.1</span>  <span class="mf">12.92</span> <span class="mf">15.1</span>  <span class="mf">14.33</span>  <span class="mf">9.67</span>  <span class="mf">9.08</span>  <span class="mf">5.64</span>
<span class="g g-Whitespace">  </span><span class="mi">6</span><span class="o">.</span><span class="mi">48</span>  <span class="mf">7.88</span><span class="p">]</span><span class="o">.</span>
<span class="n">Reshape</span> <span class="n">your</span> <span class="n">data</span> <span class="n">either</span> <span class="n">using</span> <span class="n">array</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">your</span> <span class="n">data</span> <span class="n">has</span> <span class="n">a</span> <span class="n">single</span> <span class="n">feature</span> <span class="ow">or</span> <span class="n">array</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">it</span> <span class="n">contains</span> <span class="n">a</span> <span class="n">single</span> <span class="n">sample</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
<p>What happened? If you read the error it says “Expected 2D array, got 1D array instead”. Let’s go back to the sklearn documentation. It said that the <code class="docutils literal notranslate"><span class="pre">X</span></code> should be of shape <code class="docutils literal notranslate"><span class="pre">(n_samples,</span> <span class="pre">n_features)</span></code>. Let’s look at the shape of our X.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_single</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(506,)
</pre></div>
</div>
</div>
</div>
<p>Well there’s the problem, it doesn’t have anything for <code class="docutils literal notranslate"><span class="pre">n_features</span></code>. We want to have a 1 there so that sklearn knows that there is one feature. If you remember, earlier I said that having nothing there means implicitly there is a 1. But sklearn wants you to be <em>explicit</em>, and just write the darn 1. There are several ways to do this, but the easiest way is just to access the column you want using double brackets. Rather than typing <code class="docutils literal notranslate"><span class="pre">df['my_col']</span></code>, which will results in an object with shape <code class="docutils literal notranslate"><span class="pre">(506,)</span></code> (like we just got), you can do <code class="docutils literal notranslate"><span class="pre">df[['my_col']]</span></code> (note the extra brackets), which will give the proper shape.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_single</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;pct_below_poverty_line&#39;</span><span class="p">]]</span>
<span class="n">X_single</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(506, 1)
</pre></div>
</div>
</div>
</div>
<p>Ta-da! Now let’s do everything again.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr_single</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_single</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_weak_pred</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Actual home prices&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Predicted home prices&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Actual vs predicted using linear regression&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ch_04_88_0.png" src="_images/ch_04_88_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="gradient-descent-a-id-gradient-descent-a">
<h2>Gradient descent <a id="gradient_descent"></a><a class="headerlink" href="#gradient-descent-a-id-gradient-descent-a" title="Permalink to this headline">¶</a></h2>
<p>Gradient descent is a large and important topic. This week we will look at it from the “big picture” perspective, and next week we’ll do it mathematically.</p>
<p>As we learned last week, linear regression is the process of fitting a line to data in the “best” way possible. We measured “best” in terms of mean squared error (MSE), given specifically by
$<span class="math notranslate nohighlight">\(
L(m, b) = \displaystyle\sum_i (y_i - (mx_i + b))^2
\)</span><span class="math notranslate nohighlight">\(
where \)</span>y_i<span class="math notranslate nohighlight">\( are the values to be predicted (median home value in our example last week) and \)</span>x_i<span class="math notranslate nohighlight">\( are the data being used to make predictions (things like poverty rate, distance to downtown, etc). We then showed that you could simply take derivatives and find critical points to solve for what values of \)</span>m<span class="math notranslate nohighlight">\( and \)</span>b$ will make this “error” as small as possible, i.e. minimize it.</p>
<p>None of this is common in machine learning. In fact, linear regression is largely the only case of machine learning where we can actually <em>solve</em> for what value of the <strong>model parameters</strong>, the variables used in the model, will give the smallest error. Instead, we do what is called “model training”.</p>
<p>We’ll cover model training more in-depth next week. This week, what we want to do is get the general idea of <em>how</em> we train models. The answer is “gradient descent”.</p>
<p>Recall from Calculus 3 that the gradient of a function <span class="math notranslate nohighlight">\(f\)</span>, written <span class="math notranslate nohighlight">\(\nabla f\)</span> is defined as a vector of partial derivatives. So for example, if <span class="math notranslate nohighlight">\(f(x, y) = x^2 + 2y^2\)</span>, then
$<span class="math notranslate nohighlight">\(
\nabla f(x, y) = \biggl(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\biggr) = (2x, 4y)
\)</span><span class="math notranslate nohighlight">\(
The important thing is that this is a vector, and the direction of vector is the direction in which \)</span>f<span class="math notranslate nohighlight">\( is *increasing* as fast as possible. So for instance, at the point \)</span>(2, 1, f(2, 1)) = (2, 1, 6)<span class="math notranslate nohighlight">\(, we get \)</span>\nabla f(2, 1) = (4, 4)<span class="math notranslate nohighlight">\(. What this means is that at the point \)</span>(2, 1, 6)<span class="math notranslate nohighlight">\(, \)</span>f<span class="math notranslate nohighlight">\( is increasing the fastest if we move our \)</span>(x, y)<span class="math notranslate nohighlight">\( point in the direction \)</span>(4, 4)$. Below is a visualization using <a class="reference external" href="http://www.geogebra.org">GeoGebra</a> of this surface.</p>
<p><img alt="Surface in 3D" src="https://drive.google.com/uc?id=1oZohBqWQ6AGw1MTq67fd6x4TLNQQcDE2" /></p>
<p>What the arrow (the gradient vector) is indicating is the direction your <span class="math notranslate nohighlight">\((x, y)\)</span> values should move in in order to make <span class="math notranslate nohighlight">\(f(x, y)\)</span> increase as quickly as possible.</p>
<p>So how does all this relate to training models? The idea is that we have a loss function that we want to make as small as possible. In general it is <em>not</em> possible to just take the partial derivatives and find the critical points, as the functions are too complicated. So instead, we start with a random starting point. We then compute the gradient of the loss function at that point and move in the <em>negative of the direction</em> of the gradient. Why the negative? Because the <em>positive direction</em> is where the loss function is <em>increasing</em> as quickly as possible. We don’t want it to <em>increase</em>, we want it to <em>decrease</em>. It is <em>decreasing</em> the quickest possible by moving the <em>opposite</em> (i.e. negative) direction of the gradient.</p>
<p>In order to get a visual explanation of gradient descent, please watch <a class="reference external" href="https://www.youtube.com/watch?v=sDv4f4s2SB8">this video</a>.</p>
<p>This has all been a <em>mathematical</em> explanation of the <em>idea</em> of gradient descent. But how does this actually work in Python? Luckily for all of us, everything is handled seemlessly behind the scenes. That is, we’ll call a sample function and gradient descent will “just happen.” We will go into that part next week.</p>
</div>
<div class="section" id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<p>Use the housing dataset we’ve been working with here to do the following exercises:</p>
<ol class="simple">
<li><p>Write a function which takes in a list of column names and does everything we did above. Create a scatterplot comparing the actual and predicted home values and compute the <span class="math notranslate nohighlight">\(R^2\)</span> score.</p></li>
<li><p>Figure out which single column gives the best <span class="math notranslate nohighlight">\(R^2\)</span> value. Do the same for which best two columns. Can you write a function which, for <span class="math notranslate nohighlight">\(N\)</span> columns, returns which <span class="math notranslate nohighlight">\(N\)</span> columns give the best <span class="math notranslate nohighlight">\(R^2\)</span> score?</p></li>
<li><p>In the scatterplot above there are a group of homes in the bottom left which form a horizontal line on the graph. Explain in plain English what’s going on here.</p></li>
<li><p>We’ve only been predicting the home value. Try predicting something else. Pick a column that you think is interesting and use other columns to predict it. How good are the predictions?</p></li>
<li><p>Linear regression is exactly that, linear. That means that there are no squares, square roots, sin, etc. Can you transform one of the columns in such a way that it improves predictions for home value? For example, what if instead of using distance_to_downtown you used the square of it? First, create a new distance_to_downtown_squared column, then use it in your regression.</p></li>
<li><p>We chose not to normalize our data when we did linear regression (when instantiating <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code>. Try doing it. Does it change your results?</p></li>
<li><p>Get another dataset (either from your project, or just a dataset you find interesting) and try doing all this. Linear regression is an incredibly important tool to have, and being able to work easily with it is of fundamental importance to your success.</p></li>
</ol>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="ch_03.html" title="previous page">Correlation</a>
    <a class='right-next' id="next-link" href="ch_05.html" title="next page">Training models</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By The Jupyter Book community<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>